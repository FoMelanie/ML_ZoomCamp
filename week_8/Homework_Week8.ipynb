{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877e35cf",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62b4e5",
   "metadata": {},
   "source": [
    "This notebook corresponds to the homework of the week 8 (Deep Learning) of the Machine Learning Zoomcamp (2023 cohort). The subject can be found here : https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/08-deep-learning/homework.md .\n",
    "\n",
    "The goal is to predict whether the insect on the image is a bee or a wasp with a Convolutional Neural Network (CNN) built from scratch and an image dataset having bees and wasps photos to train and test the model.\n",
    "\n",
    "<img src=\"images/maxresdefault.jpg\" style=\"display:block;float:none;margin-left:auto;margin-right:auto;width:100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835af2cd",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36949af3",
   "metadata": {},
   "source": [
    "The used dataset can be downloaded from this link : https://github.com/SVizor42/ML_Zoomcamp/releases/download/bee-wasp-data/data.zip\n",
    "It corresponds to the \"Bee or Wasp?\" Kaggle dataset that was slightly rebuilt, as specified in the homework description.\n",
    "\n",
    "To download it easily using a Saturn cloud notebook, use these commands:\n",
    "```bash\n",
    "wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/bee-wasp-data/data.zip\n",
    "unzip data.zip\n",
    "```\n",
    "\n",
    "This dataset contains two folders: train and test. Each of these two folders contains two subfolders: bee and wasp. These bee and wasp folders contain photos (.jpg format)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f99d2",
   "metadata": {},
   "source": [
    "# Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766d619",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b68cba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Melanie\\.virtualenvs\\week_8-trpDjIgx\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import statistics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "142ba593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tensorflow version\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de8781",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23b4a3",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93117d6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd2820",
   "metadata": {},
   "source": [
    "Here is how the model should be built initially:\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN). Like in the lectures, we'll use Keras.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "- The shape for input should be (150, 150, 3)\n",
    "- Next, create a convolutional layer (Conv2D):\n",
    "    - Use 32 filters\n",
    "    - Kernel size should be (3, 3) (that's the size of the filter)\n",
    "    - Use 'relu' as activation\n",
    "- Reduce the size of the feature map with max pooling (MaxPooling2D)\n",
    "    - Set the pooling size to (2, 2)\n",
    "- Turn the multi-dimensional result into vectors using a Flatten layer\n",
    "- Next, add a Dense layer with 64 neurons and 'relu' activation\n",
    "- Finally, create the Dense layer with 1 neuron - this will be the output\n",
    "    - The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "- As optimizer use SGD with the following parameters:\n",
    "    - SGD(lr=0.002, momentum=0.8)\n",
    "    \n",
    "    \n",
    "    \n",
    "For clarification about kernel size and max pooling, check Office Hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "468816ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup constants\n",
    "INPUT_SHAPE=(150,150,3)\n",
    "ACTIVATION=\"relu\"\n",
    "OUTPUT_ACTIVATION=\"sigmoid\"\n",
    "NUMBER_FILTERS=32\n",
    "KERNEL_SIZE=(3,3)\n",
    "POOLING_SIZE=(2,2)\n",
    "DENSE_FIRST_NEURONS_NUMBER=64\n",
    "DENSE_OUTPUT_NEURONS_NUMBER=1\n",
    "SGD_LR = 0.002\n",
    "SGD_MOMENTUM = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2f914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Melanie\\.virtualenvs\\week_8-trpDjIgx\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Melanie\\.virtualenvs\\week_8-trpDjIgx\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(filters = NUMBER_FILTERS,\n",
    "                      kernel_size=KERNEL_SIZE,\n",
    "                      activation=ACTIVATION,\n",
    "                      input_shape=INPUT_SHAPE),\n",
    "        layers.MaxPooling2D(pool_size=POOLING_SIZE),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=DENSE_FIRST_NEURONS_NUMBER, \n",
    "                     activation=ACTIVATION),\n",
    "        layers.Dense(units=DENSE_OUTPUT_NEURONS_NUMBER, \n",
    "                     activation=OUTPUT_ACTIVATION)\n",
    "    ]\n",
    ")\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "opt = keras.optimizers.SGD(learning_rate=SGD_LR,\n",
    "                           momentum = SGD_MOMENTUM)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693b4c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 175232)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                11214912  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11215873 (42.79 MB)\n",
      "Trainable params: 11215873 (42.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25137b",
   "metadata": {},
   "source": [
    "The Conv2D layer has 896 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64934856",
   "metadata": {},
   "source": [
    "### Training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534c04f",
   "metadata": {},
   "source": [
    "- We don't need to do any additional pre-processing for the images.\n",
    "- When reading the data from train/test directories, check the class_mode parameter. Which value should it be for a binary classification problem?\n",
    "- Use batch_size=20\n",
    "- Use shuffle=True for both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d3b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator \n",
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8c663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target size (according to input shape)\n",
    "TARGET_SIZE=(150,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c136e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = datagen.flow_from_directory(\n",
    "    './data/train/',\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    class_mode='binary',\n",
    "    target_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b9f821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 918 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = datagen.flow_from_directory(\n",
    "    './data/test/',\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    class_mode='binary',\n",
    "    target_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e3757",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc661459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Melanie\\.virtualenvs\\week_8-trpDjIgx\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Melanie\\.virtualenvs\\week_8-trpDjIgx\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.6513 - accuracy: 0.6032 - val_loss: 0.5797 - val_accuracy: 0.7168\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 26s 140ms/step - loss: 0.5658 - accuracy: 0.7134 - val_loss: 0.5692 - val_accuracy: 0.6972\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 27s 145ms/step - loss: 0.5349 - accuracy: 0.7395 - val_loss: 0.6219 - val_accuracy: 0.6492\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 27s 146ms/step - loss: 0.5096 - accuracy: 0.7599 - val_loss: 0.5350 - val_accuracy: 0.7288\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 26s 140ms/step - loss: 0.4720 - accuracy: 0.7892 - val_loss: 0.5792 - val_accuracy: 0.7081\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 26s 142ms/step - loss: 0.4610 - accuracy: 0.7922 - val_loss: 0.5414 - val_accuracy: 0.7386\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 26s 144ms/step - loss: 0.4332 - accuracy: 0.8132 - val_loss: 0.5208 - val_accuracy: 0.7429\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 26s 143ms/step - loss: 0.4015 - accuracy: 0.8330 - val_loss: 0.4968 - val_accuracy: 0.7636\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 26s 141ms/step - loss: 0.3677 - accuracy: 0.8455 - val_loss: 0.5013 - val_accuracy: 0.7614\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 26s 142ms/step - loss: 0.3299 - accuracy: 0.8773 - val_loss: 0.5782 - val_accuracy: 0.7353\n"
     ]
    }
   ],
   "source": [
    "result = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90b3261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6512953042984009,\n",
       "  0.5657526254653931,\n",
       "  0.5348544716835022,\n",
       "  0.5096314549446106,\n",
       "  0.4719887673854828,\n",
       "  0.46101152896881104,\n",
       "  0.43318313360214233,\n",
       "  0.4015069901943207,\n",
       "  0.3677012026309967,\n",
       "  0.329925537109375],\n",
       " 'accuracy': [0.6032091379165649,\n",
       "  0.7133532762527466,\n",
       "  0.7394615411758423,\n",
       "  0.7598586082458496,\n",
       "  0.7892303466796875,\n",
       "  0.7922219038009644,\n",
       "  0.8131629228591919,\n",
       "  0.833016037940979,\n",
       "  0.8455262184143066,\n",
       "  0.8773456811904907],\n",
       " 'val_loss': [0.5797104239463806,\n",
       "  0.5692185163497925,\n",
       "  0.621886670589447,\n",
       "  0.5350314974784851,\n",
       "  0.5792214274406433,\n",
       "  0.5414077639579773,\n",
       "  0.520801842212677,\n",
       "  0.49683448672294617,\n",
       "  0.501284122467041,\n",
       "  0.5782131552696228],\n",
       " 'val_accuracy': [0.7167755961418152,\n",
       "  0.6971677541732788,\n",
       "  0.6492374539375305,\n",
       "  0.7287581562995911,\n",
       "  0.7080609798431396,\n",
       "  0.7385621070861816,\n",
       "  0.742919385433197,\n",
       "  0.7636165618896484,\n",
       "  0.7614378929138184,\n",
       "  0.7352941036224365]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99989e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907261252403259"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Median of training accuracy\n",
    "statistics.median(result.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "021fd951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0965853857147639"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard deviation of training loss\n",
    "statistics.stdev(result.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e6642",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f3cc0",
   "metadata": {},
   "source": [
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "- rotation_range=50,\n",
    "- width_shift_range=0.1,\n",
    "- height_shift_range=0.1,\n",
    "- zoom_range=0.1,\n",
    "- horizontal_flip=True,\n",
    "- fill_mode='nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53bb74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator for data augmentation\n",
    "datagen_augmentation = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=50,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.1,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fa7fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_augmentation = datagen_augmentation.flow_from_directory('./data/train/',\n",
    "                                                    target_size=(150, 150), \n",
    "                                                    batch_size=32, \n",
    "                                                    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42400487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "115/115 [==============================] - 31s 270ms/step - loss: 0.4951 - accuracy: 0.7707 - val_loss: 0.5457 - val_accuracy: 0.7625\n",
      "Epoch 2/10\n",
      "115/115 [==============================] - 19s 168ms/step - loss: 0.4808 - accuracy: 0.7800 - val_loss: 0.4659 - val_accuracy: 0.7898\n",
      "Epoch 3/10\n",
      "115/115 [==============================] - 19s 165ms/step - loss: 0.4709 - accuracy: 0.7876 - val_loss: 0.4782 - val_accuracy: 0.7734\n",
      "Epoch 4/10\n",
      "115/115 [==============================] - 21s 180ms/step - loss: 0.4694 - accuracy: 0.7857 - val_loss: 0.4621 - val_accuracy: 0.7930\n",
      "Epoch 5/10\n",
      "115/115 [==============================] - 20s 176ms/step - loss: 0.4588 - accuracy: 0.7871 - val_loss: 0.4666 - val_accuracy: 0.7778\n",
      "Epoch 6/10\n",
      "115/115 [==============================] - 33s 292ms/step - loss: 0.4652 - accuracy: 0.7819 - val_loss: 0.4706 - val_accuracy: 0.7876\n",
      "Epoch 7/10\n",
      "115/115 [==============================] - 36s 309ms/step - loss: 0.4669 - accuracy: 0.7811 - val_loss: 0.4729 - val_accuracy: 0.7789\n",
      "Epoch 8/10\n",
      "115/115 [==============================] - 35s 304ms/step - loss: 0.4571 - accuracy: 0.7968 - val_loss: 0.4488 - val_accuracy: 0.7876\n",
      "Epoch 9/10\n",
      "115/115 [==============================] - 34s 297ms/step - loss: 0.4476 - accuracy: 0.7958 - val_loss: 0.4507 - val_accuracy: 0.7887\n",
      "Epoch 10/10\n",
      "115/115 [==============================] - 36s 314ms/step - loss: 0.4511 - accuracy: 0.7971 - val_loss: 0.4741 - val_accuracy: 0.7756\n"
     ]
    }
   ],
   "source": [
    "result_augmentation = model.fit(\n",
    "    train_augmentation,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7996dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.49508029222488403,\n",
       "  0.4808209240436554,\n",
       "  0.47093522548675537,\n",
       "  0.4693944752216339,\n",
       "  0.4588375389575958,\n",
       "  0.46520674228668213,\n",
       "  0.46692657470703125,\n",
       "  0.45710718631744385,\n",
       "  0.44762396812438965,\n",
       "  0.4511086642742157],\n",
       " 'accuracy': [0.7707369923591614,\n",
       "  0.7799836993217468,\n",
       "  0.7875986099243164,\n",
       "  0.7856948375701904,\n",
       "  0.7870546579360962,\n",
       "  0.781887412071228,\n",
       "  0.7810715436935425,\n",
       "  0.7968452572822571,\n",
       "  0.7957574129104614,\n",
       "  0.7971172332763672],\n",
       " 'val_loss': [0.5457229614257812,\n",
       "  0.4659121036529541,\n",
       "  0.47818687558174133,\n",
       "  0.4620767831802368,\n",
       "  0.46662330627441406,\n",
       "  0.4705820381641388,\n",
       "  0.4729148745536804,\n",
       "  0.44878822565078735,\n",
       "  0.45068982243537903,\n",
       "  0.47412049770355225],\n",
       " 'val_accuracy': [0.7625272274017334,\n",
       "  0.7897603511810303,\n",
       "  0.7734204530715942,\n",
       "  0.7930282950401306,\n",
       "  0.7777777910232544,\n",
       "  0.7875816822052002,\n",
       "  0.7788671255111694,\n",
       "  0.7875816822052002,\n",
       "  0.7886710166931152,\n",
       "  0.7755991220474243]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_augmentation.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "271f36c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47356174886226654"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean of test loss\n",
    "statistics.mean(result_augmentation.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eb609bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836601257324218"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average of test accuracy for the last 5 epochs\n",
    "statistics.mean(result_augmentation.history['val_accuracy'][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469724d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
