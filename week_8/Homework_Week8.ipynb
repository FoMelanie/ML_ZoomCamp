{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877e35cf",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62b4e5",
   "metadata": {},
   "source": [
    "This notebook corresponds to the homework of the week 8 (Deep Learning) of the Machine Learning Zoomcamp (2023 cohort). The subject can be found here : https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/08-deep-learning/homework.md .\n",
    "\n",
    "The goal is to predict whether the insect on the image is a bee or a wasp with a Convolutional Neural Network (CNN) built from scratch and an image dataset having bees and wasps photos to train and test the model.\n",
    "\n",
    "<img src=\"images/maxresdefault.jpg\" style=\"display:block;float:none;margin-left:auto;margin-right:auto;width:100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835af2cd",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36949af3",
   "metadata": {},
   "source": [
    "The used dataset can be downloaded from this link : https://github.com/SVizor42/ML_Zoomcamp/releases/download/bee-wasp-data/data.zip\n",
    "It corresponds to the \"Bee or Wasp?\" Kaggle dataset that was slightly rebuilt, as specified in the homework description.\n",
    "\n",
    "To download it easily using a Saturn cloud notebook, use these commands:\n",
    "```bash\n",
    "wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/bee-wasp-data/data.zip\n",
    "unzip data.zip\n",
    "```\n",
    "\n",
    "This dataset contains two folders: train and test. Each of these two folders contains two subfolders: bee and wasp. These bee and wasp folders contain photos (.jpg format)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f99d2",
   "metadata": {},
   "source": [
    "# Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766d619",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b68cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142ba593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tensorflow version\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de8781",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23b4a3",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93117d6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd2820",
   "metadata": {},
   "source": [
    "Here is how the model should be built initially:\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN). Like in the lectures, we'll use Keras.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "- The shape for input should be (150, 150, 3)\n",
    "- Next, create a convolutional layer (Conv2D):\n",
    "    - Use 32 filters\n",
    "    - Kernel size should be (3, 3) (that's the size of the filter)\n",
    "    - Use 'relu' as activation\n",
    "- Reduce the size of the feature map with max pooling (MaxPooling2D)\n",
    "    - Set the pooling size to (2, 2)\n",
    "- Turn the multi-dimensional result into vectors using a Flatten layer\n",
    "- Next, add a Dense layer with 64 neurons and 'relu' activation\n",
    "- Finally, create the Dense layer with 1 neuron - this will be the output\n",
    "    - The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "- As optimizer use SGD with the following parameters:\n",
    "    - SGD(lr=0.002, momentum=0.8)\n",
    "    \n",
    "    \n",
    "    \n",
    "For clarification about kernel size and max pooling, check Office Hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "468816ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup constants\n",
    "INPUT_SHAPE=(150,150,3)\n",
    "ACTIVATION=\"relu\"\n",
    "OUTPUT_ACTIVATION=\"sigmoid\"\n",
    "NUMBER_FILTERS=32\n",
    "KERNEL_SIZE=(3,3)\n",
    "POOLING_SIZE=(2,2)\n",
    "DENSE_FIRST_NEURONS_NUMBER=64\n",
    "DENSE_OUTPUT_NEURONS_NUMBER=1\n",
    "SGD_LR = 0.002\n",
    "SGD_MOMENTUM = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9b2f914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(filters = NUMBER_FILTERS,\n",
    "                      kernel_size=KERNEL_SIZE,\n",
    "                      activation=ACTIVATION,\n",
    "                      input_shape=INPUT_SHAPE),\n",
    "        layers.MaxPooling2D(pool_size=POOLING_SIZE),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=DENSE_FIRST_NEURONS_NUMBER, \n",
    "                     activation=ACTIVATION),\n",
    "        layers.Dense(units=DENSE_OUTPUT_NEURONS_NUMBER, \n",
    "                     activation=OUTPUT_ACTIVATION)\n",
    "    ]\n",
    ")\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "opt = keras.optimizers.SGD(learning_rate=SGD_LR,\n",
    "                           momentum = SGD_MOMENTUM)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "693b4c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 74, 74, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 175232)            0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                11214912  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,215,873\n",
      "Trainable params: 11,215,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25137b",
   "metadata": {},
   "source": [
    "The Conv2D layer has 896 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64934856",
   "metadata": {},
   "source": [
    "### Training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534c04f",
   "metadata": {},
   "source": [
    "- We don't need to do any additional pre-processing for the images.\n",
    "- When reading the data from train/test directories, check the class_mode parameter. Which value should it be for a binary classification problem?\n",
    "- Use batch_size=20\n",
    "- Use shuffle=True for both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70d3b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator \n",
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c8c663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target size (according to input shape)\n",
    "TARGET_SIZE=(150,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c136e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = datagen.flow_from_directory(\n",
    "    './data/train/',\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    class_mode='binary',\n",
    "    target_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31b9f821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 918 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = datagen.flow_from_directory(\n",
    "    './data/test/',\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    class_mode='binary',\n",
    "    target_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e3757",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc661459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 9s 47ms/step - loss: 0.6863 - accuracy: 0.5423 - val_loss: 0.6476 - val_accuracy: 0.5937\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 8s 46ms/step - loss: 0.6567 - accuracy: 0.5902 - val_loss: 0.6191 - val_accuracy: 0.6481\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 8s 46ms/step - loss: 0.6196 - accuracy: 0.6454 - val_loss: 0.6056 - val_accuracy: 0.6612\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 9s 46ms/step - loss: 0.5731 - accuracy: 0.6943 - val_loss: 0.6480 - val_accuracy: 0.5850\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 8s 46ms/step - loss: 0.5408 - accuracy: 0.7278 - val_loss: 0.5403 - val_accuracy: 0.7277\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 8s 46ms/step - loss: 0.5129 - accuracy: 0.7580 - val_loss: 0.5368 - val_accuracy: 0.7571\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 8s 46ms/step - loss: 0.4979 - accuracy: 0.7716 - val_loss: 0.5488 - val_accuracy: 0.7113\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 8s 46ms/step - loss: 0.4836 - accuracy: 0.7764 - val_loss: 0.5406 - val_accuracy: 0.7309\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 8s 46ms/step - loss: 0.4620 - accuracy: 0.7963 - val_loss: 0.5251 - val_accuracy: 0.7571\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 8s 46ms/step - loss: 0.4404 - accuracy: 0.8123 - val_loss: 0.5441 - val_accuracy: 0.7418\n"
     ]
    }
   ],
   "source": [
    "result = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b90b3261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6862706542015076,\n",
       "  0.6566957235336304,\n",
       "  0.6195502877235413,\n",
       "  0.5731316208839417,\n",
       "  0.5408139228820801,\n",
       "  0.5128502249717712,\n",
       "  0.49793028831481934,\n",
       "  0.48356375098228455,\n",
       "  0.4619581699371338,\n",
       "  0.44041910767555237],\n",
       " 'accuracy': [0.5422899127006531,\n",
       "  0.5901550054550171,\n",
       "  0.6453630924224854,\n",
       "  0.694316029548645,\n",
       "  0.7277672290802002,\n",
       "  0.7579548358917236,\n",
       "  0.7715529203414917,\n",
       "  0.7764481902122498,\n",
       "  0.7963013052940369,\n",
       "  0.8123469948768616],\n",
       " 'val_loss': [0.6475895047187805,\n",
       "  0.6191149353981018,\n",
       "  0.6055774688720703,\n",
       "  0.6480372548103333,\n",
       "  0.5402923822402954,\n",
       "  0.5368143916130066,\n",
       "  0.5487772226333618,\n",
       "  0.5405684113502502,\n",
       "  0.5251412391662598,\n",
       "  0.5441334247589111],\n",
       " 'val_accuracy': [0.5936819314956665,\n",
       "  0.6481481194496155,\n",
       "  0.6612200140953064,\n",
       "  0.584967315196991,\n",
       "  0.727668821811676,\n",
       "  0.757080614566803,\n",
       "  0.7113289833068848,\n",
       "  0.7309368252754211,\n",
       "  0.757080614566803,\n",
       "  0.741830050945282]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99989e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7428610324859619"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Median of training accuracy\n",
    "statistics.median(result.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "021fd951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08406540282090642"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard deviation of training loss\n",
    "statistics.stdev(result.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e6642",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f3cc0",
   "metadata": {},
   "source": [
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "- rotation_range=50,\n",
    "- width_shift_range=0.1,\n",
    "- height_shift_range=0.1,\n",
    "- zoom_range=0.1,\n",
    "- horizontal_flip=True,\n",
    "- fill_mode='nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53bb74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator for data augmentation\n",
    "datagen_augmentation = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=50,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.1,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6fa7fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_augmentation = datagen_augmentation.flow_from_directory('./data/train/',\n",
    "                                                    target_size=(150, 150), \n",
    "                                                    batch_size=32, \n",
    "                                                    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "42400487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 0.5051 - accuracy: 0.7593 - val_loss: 0.5198 - val_accuracy: 0.7484\n",
      "Epoch 2/10\n",
      "115/115 [==============================] - 24s 206ms/step - loss: 0.4948 - accuracy: 0.7778 - val_loss: 0.5039 - val_accuracy: 0.7680\n",
      "Epoch 3/10\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 0.4873 - accuracy: 0.7754 - val_loss: 0.4867 - val_accuracy: 0.7832\n",
      "Epoch 4/10\n",
      "115/115 [==============================] - 24s 206ms/step - loss: 0.4963 - accuracy: 0.7680 - val_loss: 0.5041 - val_accuracy: 0.7495\n",
      "Epoch 5/10\n",
      "115/115 [==============================] - 24s 206ms/step - loss: 0.4902 - accuracy: 0.7707 - val_loss: 0.4991 - val_accuracy: 0.7658\n",
      "Epoch 6/10\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 0.4818 - accuracy: 0.7743 - val_loss: 0.4831 - val_accuracy: 0.7691\n",
      "Epoch 7/10\n",
      "115/115 [==============================] - 24s 206ms/step - loss: 0.4677 - accuracy: 0.7841 - val_loss: 0.5363 - val_accuracy: 0.7386\n",
      "Epoch 8/10\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 0.4676 - accuracy: 0.7830 - val_loss: 0.4673 - val_accuracy: 0.7789\n",
      "Epoch 9/10\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 0.4648 - accuracy: 0.7922 - val_loss: 0.5070 - val_accuracy: 0.7473\n",
      "Epoch 10/10\n",
      "115/115 [==============================] - 24s 206ms/step - loss: 0.4584 - accuracy: 0.7884 - val_loss: 0.4916 - val_accuracy: 0.7658\n"
     ]
    }
   ],
   "source": [
    "result_augmentation = model.fit(\n",
    "    train_augmentation,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7996dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.505081057548523,\n",
       "  0.4947676658630371,\n",
       "  0.4872701168060303,\n",
       "  0.49634361267089844,\n",
       "  0.49021443724632263,\n",
       "  0.48182985186576843,\n",
       "  0.46772652864456177,\n",
       "  0.46757882833480835,\n",
       "  0.4648420512676239,\n",
       "  0.458371102809906],\n",
       " 'accuracy': [0.7593146562576294,\n",
       "  0.7778080105781555,\n",
       "  0.7753603458404541,\n",
       "  0.7680174112319946,\n",
       "  0.7707369923591614,\n",
       "  0.7742725014686584,\n",
       "  0.7840631008148193,\n",
       "  0.7829752564430237,\n",
       "  0.7922219038009644,\n",
       "  0.788414478302002],\n",
       " 'val_loss': [0.5197620987892151,\n",
       "  0.503851592540741,\n",
       "  0.48665040731430054,\n",
       "  0.5040733814239502,\n",
       "  0.499086856842041,\n",
       "  0.4831225574016571,\n",
       "  0.5363031625747681,\n",
       "  0.4673035442829132,\n",
       "  0.5069840550422668,\n",
       "  0.4916398525238037],\n",
       " 'val_accuracy': [0.7483659982681274,\n",
       "  0.7679738402366638,\n",
       "  0.7832244038581848,\n",
       "  0.7494553327560425,\n",
       "  0.7657952308654785,\n",
       "  0.7690631747245789,\n",
       "  0.7385621070861816,\n",
       "  0.7788671255111694,\n",
       "  0.7472766637802124,\n",
       "  0.7657952308654785]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_augmentation.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "271f36c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4998777508735657"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean of test loss\n",
    "statistics.mean(result_augmentation.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3eb609bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7599128603935241"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average of test accuracy for the last 5 epochs\n",
    "statistics.mean(result_augmentation.history['val_accuracy'][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469724d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
